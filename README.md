Procedure followed for question 1: 
Shark Tank Tables- 
Here we use Selenium WebDriver to automate the process of opening the given url in the web browser(Firefox in this case) through program. Then we use a popular Python library called BeautifulSoup, that parses html pages by creating a parse tree that can be used to extract the data from the given page. This is then followed by closely inspecting the HTML elements that make up the page. For extracting the given table, we use indexing to access it from the list of tables and then move on to find the column headings and the table body contents for the desired table. We then store the information that is extracted from the above steps(table headings and table body) in a list, which is then converted to a Pandas dataframe. We have also written a reusable function code to scrape the table for all the columns and the dataframe thus obtained is then converted to a csv file.
Procedure followed for question 2: 
To scrape data from a dynamic e-commerce website for the given Macbook products- 
This procedure also involves the use of Selenium webdriver and BeautifulSoup library to automate the process of opening the given url and to parse the HTML page for data extraction, respectively. Once the required columns i.e 1. Product Name 2. Price: Integer 3. Rating out of 5 4. Total no of Rating 5. Total no of Reviews 6. List of all the device specification- are extracted with their respective tags, we first check if the size of the columns(length) are the same or not to verify our extraction process. Then we write a reusable function for all the columns and return an array consisting of the extracted information alongwith appropriate headings. Since there are 25 products in total and we have only 24 products listed in the 1st page, we extract the 2nd page of the given url, too. Thus the data from all the pages and compiled into a single dataframe using concat method. The datatype of the columns in the final dataframe is changed as per requirement and finally it is converted to a csv file.
Procedure followed for question 3: 
To scrape data from a static website-
This procedure also involves the use of Selenium webdriver and BeautifulSoup library to automate the process of opening the given url and to parse the HTML page for data extraction, respectively. However here out of the 6 required columns i.e. 1. Employee Name 2. Job Position 3. Working 4. Interview quote 5. Date 6. Tags, 3 have one common parent tag while the other 3 have another common parent tag, which makes distinguishing between them difficult. So indexing and slicing methods are used to extract the desired information. We move on to write a reusable function to extract the information for 5 such pages, we get 25 rows of data. We also observe that the 1st entry has 1 missing data, and so the respective column for the 1st row is filled with NaN value.The datatype of the columns in the final dataframe is changed as per requirement and finally it is converted to a csv file.
